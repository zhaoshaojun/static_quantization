{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-training static quantization (Pytorch) - ResNet18\n",
    "In this notebook, you will be able to see how quantization in PyTorch can result in significant decreases in model size while increasing speed. Note that quantization is currently only supported for CPUs, so we will be utilizing GPUs / CUDA only for training and CPU for testing.\n",
    "Furthermore, while using complex dataset the accuracy might decrease upon quantization. By using a quantization configuration\n",
    "\n",
    "    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "we can significantly improve on the accuracy. We repeat the same exercise with the recommended configuration for quantizing for x86 architectures. This configuration does the following:\n",
    "1. Quantizes weights on a per-channel basis\n",
    "2. Uses a histogram observer that collects a histogram of activations and then picks quantization parameters in an optimal manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.utils.bottleneck as B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and visualize MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: ../data: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(torchvision.datasets.MNIST('../data', train=True, download=True,\n",
    "                                                    transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                                    ])),\n",
    "               batch_size=64, shuffle=True, num_workers=1, pin_memory=True)\n",
    "\n",
    "test_loader = DataLoader(torchvision.datasets.MNIST('../data', train=False, \n",
    "                                                    transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                                    ])),\n",
    "              batch_size=64, shuffle=True, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "figure = plt.figure()\n",
    "num_of_images = 20\n",
    "for index in range(1, num_of_images + 1):\n",
    "    plt.subplot(6, 10, index)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet18 model\n",
    "This code is taken from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py \n",
    "\n",
    "> NOTE: Training uses resnet model as is with addition operation and floating point inputs / outputs.      \n",
    "But when model is quantized while testing addition operation is replaced with FloatFunction and the inputs         / outputs are quantized/dequantized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\n",
    "    \n",
    "    Args:\n",
    "        in_planes: number of channels in input image\n",
    "        out_planes: number of channels produced by convolution\n",
    "        stride: stride of the convolution. Default: 1\n",
    "        groups: Number of blocked connections from input channels to output channels. Default: 1\n",
    "        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
    "        \n",
    "    Returns:\n",
    "        Convoluted layer of kernel size=3, with specified out_planes\n",
    "    \n",
    "    \"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\n",
    "    \n",
    "    Args:\n",
    "        in_planes: number of channels in input image\n",
    "        out_planes: number of channels produced by convolution\n",
    "        stride: stride of the convolution. Default: 1\n",
    "        \n",
    "    Returns:\n",
    "        Convoluted layer of kernel size=1, with specified out_planes\n",
    "        \n",
    "    \"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None, quantize=False):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.quantize = quantize\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        # Notice the addition operation in both scenarios\n",
    "        if self.quantize:\n",
    "            out = self.skip_add.add(out, identity)\n",
    "        else:\n",
    "            out += identity\n",
    "\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None, quantize=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.quantize = quantize\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        # FloatFunction()\n",
    "        self.skip_add = nn.quantized.FloatFunctional()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        # Notice the addition operation in both scenarios\n",
    "        if self.quantize:\n",
    "            out = self.skip_add.add(out, identity)\n",
    "        else:\n",
    "            out += identity\n",
    "\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-18 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet34(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-34 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet50(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-50 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,\n",
    "                   **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block=BasicBlock, layers=[2, 2, 2, 2], num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None, mnist=False, quantize=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.quantize = quantize\n",
    "        if mnist:\n",
    "            num_channels = 1\n",
    "        else:\n",
    "            num_channels = 3\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace \n",
    "            # the 2x2 stride with a dilated convolution instead.\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(num_channels, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer, quantize=self.quantize))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer, quantize=self.quantize))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # Input are quantized\n",
    "        if self.quantize:\n",
    "            x = self.quant(x)\n",
    "    \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Outputs are dequantized\n",
    "        if self.quantize:\n",
    "            x = self.dequant(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "         # See note [TorchScript super()]\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\" Train the model with given dataset\n",
    "    \n",
    "    Args:\n",
    "        args: args like log interval\n",
    "        model: ResNet model to train\n",
    "        device: CPU/GPU\n",
    "        train_loader: dataset iterator\n",
    "        optimizer: optimizer to update weights\n",
    "        epoch: number of epochs to train for\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(F.log_softmax(output, dim=-1), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % args[\"log_interval\"] == 0:\n",
    "            print('{} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                datetime.now(),\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_old():\n",
    " \n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    lr = 0.01\n",
    "    momentum = 0.5\n",
    "    seed = 1\n",
    "    log_interval = 5\n",
    "    save_model = True\n",
    "    no_cuda = False\n",
    "    \n",
    "    use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    model = ResNet(block=BasicBlock, layers=[2, 2, 2, 2], num_classes=10, mnist=True).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    args = {}\n",
    "    args[\"log_interval\"] = log_interval\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "\n",
    "    if (save_model):\n",
    "        torch.save(model.state_dict(),\"mnist_cnn.pt\")\n",
    "\n",
    "# main_old()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    " \n",
    "    batch_size = 64\n",
    "    epochs = 1\n",
    "    lr = 0.01\n",
    "    momentum = 0.5\n",
    "    seed = 1\n",
    "    log_interval = 5\n",
    "    save_model = True\n",
    "    no_cuda = False\n",
    "    \n",
    "    use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    model = ResNet(block=Bottleneck, layers=[3, 4, 6, 3], num_classes=10, mnist=True).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    args = {}\n",
    "    args[\"log_interval\"] = log_interval\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "\n",
    "    if (save_model):\n",
    "        torch.save(model.state_dict(),\"mnist_cnn.pt\")\n",
    "\n",
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model):\n",
    "    \"\"\" Print the size of the model.\n",
    "    \n",
    "    Args:\n",
    "        model: model whose size needs to be determined\n",
    "\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size of the model(MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, quantize=False, fbgemm=False):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Testing with qauntization if quantize=True\n",
    "    if quantize:\n",
    "        modules_to_fuse_old = [['conv1', 'bn1'],\n",
    "                   ['layer1.0.conv1', 'layer1.0.bn1'],\n",
    "                   ['layer1.0.conv2', 'layer1.0.bn2'],\n",
    "                   ['layer1.1.conv1', 'layer1.1.bn1'],\n",
    "                   ['layer1.1.conv2', 'layer1.1.bn2'],\n",
    "                   ['layer2.0.conv1', 'layer2.0.bn1'],\n",
    "                   ['layer2.0.conv2', 'layer2.0.bn2'],\n",
    "                   ['layer2.0.downsample.0', 'layer2.0.downsample.1'],\n",
    "                   ['layer2.1.conv1', 'layer2.1.bn1'],\n",
    "                   ['layer2.1.conv2', 'layer2.1.bn2'],\n",
    "                   ['layer3.0.conv1', 'layer3.0.bn1'],\n",
    "                   ['layer3.0.conv2', 'layer3.0.bn2'],\n",
    "                   ['layer3.0.downsample.0', 'layer3.0.downsample.1'],\n",
    "                   ['layer3.1.conv1', 'layer3.1.bn1'],\n",
    "                   ['layer3.1.conv2', 'layer3.1.bn2'],\n",
    "                   ['layer4.0.conv1', 'layer4.0.bn1'],\n",
    "                   ['layer4.0.conv2', 'layer4.0.bn2'],\n",
    "                   ['layer4.0.downsample.0', 'layer4.0.downsample.1'],\n",
    "                   ['layer4.1.conv1', 'layer4.1.bn1'],\n",
    "                   ['layer4.1.conv2', 'layer4.1.bn2']]\n",
    "        model = torch.quantization.fuse_modules(model, modules_to_fuse_old)\n",
    "        if fbgemm:\n",
    "            model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "        else:\n",
    "            model.qconfig = torch.quantization.default_qconfig\n",
    "        torch.quantization.prepare(model, inplace=True)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data, target in train_loader:\n",
    "                model(data)\n",
    "        torch.quantization.convert(model, inplace=True)\n",
    "\n",
    "    print(model)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    index = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            index += 1\n",
    "            print(index, datetime.now())\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            st = time.time()\n",
    "            output = model(data)\n",
    "            et = time.time()\n",
    "            test_loss += F.nll_loss(F.log_softmax(output, dim=-1), target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print(\"========================================= PERFORMANCE =============================================\")\n",
    "    print_size_of_model(model)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    print('Elapsed time = {:0.4f} milliseconds'.format((et - st) * 1000))\n",
    "    print(\"====================================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline performance - unquantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "encoder = ResNet(num_classes=10, mnist=True)\n",
    "loaded_dict_enc = torch.load('mnist_cnn-resnet18.pt', map_location=device)\n",
    "encoder.load_state_dict(loaded_dict_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "========================================= PERFORMANCE =============================================\n",
      "Size of the model(MB): 44.782195\n",
      "\n",
      "Test set: Average loss: 0.0325, Accuracy: 9897/10000 (99%)\n",
      "\n",
      "Elapsed time = 28.4178 milliseconds\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "test(model=encoder, device=device, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): QuantizedConv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.12347077578306198, zero_point=61, padding=(3, 3))\n",
      "  (bn1): Identity()\n",
      "  (relu): QuantizedReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.10298416018486023, zero_point=60, padding=(1, 1))\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(inplace=True)\n",
      "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.10945683717727661, zero_point=61, padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (skip_add): QFunctional(\n",
      "        scale=0.13676385581493378, zero_point=47\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.09816549718379974, zero_point=61, padding=(1, 1))\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(inplace=True)\n",
      "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.10349876433610916, zero_point=62, padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (skip_add): QFunctional(\n",
      "        scale=0.15333621203899384, zero_point=42\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.11402199417352676, zero_point=66, padding=(1, 1))\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(inplace=True)\n",
      "      (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.09683991968631744, zero_point=63, padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.1252516359090805, zero_point=57)\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (skip_add): QFunctional(\n",
      "        scale=0.18063707649707794, zero_point=55\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.10193919390439987, zero_point=63, padding=(1, 1))\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(inplace=True)\n",
      "      (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.10610567033290863, zero_point=63, padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (skip_add): QFunctional(\n",
      "        scale=0.1506330966949463, zero_point=41\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.10301125049591064, zero_point=64, padding=(1, 1))\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(inplace=True)\n",
      "      (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.07678677886724472, zero_point=63, padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.12266261130571365, zero_point=66)\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (skip_add): QFunctional(\n",
      "        scale=0.13961999118328094, zero_point=60\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.07777702808380127, zero_point=63, padding=(1, 1))\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(inplace=True)\n",
      "      (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.08191445469856262, zero_point=62, padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (skip_add): QFunctional(\n",
      "        scale=0.12591896951198578, zero_point=40\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.08079221844673157, zero_point=64, padding=(1, 1))\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(inplace=True)\n",
      "      (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.08426522463560104, zero_point=64, padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.08299878984689713, zero_point=60)\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (skip_add): QFunctional(\n",
      "        scale=0.12526896595954895, zero_point=56\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.0760582759976387, zero_point=63, padding=(1, 1))\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(inplace=True)\n",
      "      (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.08169588446617126, zero_point=64, padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (skip_add): QFunctional(\n",
      "        scale=0.12883007526397705, zero_point=41\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): QuantizedLinear(in_features=512, out_features=10, scale=0.24648116528987885, zero_point=34, qscheme=torch.per_tensor_affine)\n",
      "  (quant): Quantize(scale=tensor([0.0256]), zero_point=tensor([17]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      ")\n",
      "========================================= PERFORMANCE =============================================\n",
      "Size of the model(MB): 11.220779\n",
      "\n",
      "Test set: Average loss: 0.0338, Accuracy: 9891/10000 (99%)\n",
      "\n",
      "Elapsed time = 11.2729 milliseconds\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "encoder = ResNet(num_classes=10, mnist=True, quantize=True)\n",
    "loaded_dict_enc = torch.load('mnist_cnn-resnet18.pt', map_location=device)\n",
    "encoder.load_state_dict(loaded_dict_enc)\n",
    "test(model=encoder, device=device, test_loader=test_loader, quantize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16/40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "encoder2 = ResNet(num_classes=10, mnist=True)\n",
    "loaded_dict_enc = torch.load('mnist_cnn.pt', map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tUnexpected key(s) in state_dict: \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.bn1.bias\", \"layer1.2.bn1.running_mean\", \"layer1.2.bn1.running_var\", \"layer1.2.bn1.num_batches_tracked\", \"layer1.2.conv2.weight\", \"layer1.2.bn2.weight\", \"layer1.2.bn2.bias\", \"layer1.2.bn2.running_mean\", \"layer1.2.bn2.running_var\", \"layer1.2.bn2.num_batches_tracked\", \"layer1.2.conv3.weight\", \"layer1.2.bn3.weight\", \"layer1.2.bn3.bias\", \"layer1.2.bn3.running_mean\", \"layer1.2.bn3.running_var\", \"layer1.2.bn3.num_batches_tracked\", \"layer1.0.conv3.weight\", \"layer1.0.bn3.weight\", \"layer1.0.bn3.bias\", \"layer1.0.bn3.running_mean\", \"layer1.0.bn3.running_var\", \"layer1.0.bn3.num_batches_tracked\", \"layer1.0.downsample.0.weight\", \"layer1.0.downsample.1.weight\", \"layer1.0.downsample.1.bias\", \"layer1.0.downsample.1.running_mean\", \"layer1.0.downsample.1.running_var\", \"layer1.0.downsample.1.num_batches_tracked\", \"layer1.1.conv3.weight\", \"layer1.1.bn3.weight\", \"layer1.1.bn3.bias\", \"layer1.1.bn3.running_mean\", \"layer1.1.bn3.running_var\", \"layer1.1.bn3.num_batches_tracked\", \"layer2.2.conv1.weight\", \"layer2.2.bn1.weight\", \"layer2.2.bn1.bias\", \"layer2.2.bn1.running_mean\", \"layer2.2.bn1.running_var\", \"layer2.2.bn1.num_batches_tracked\", \"layer2.2.conv2.weight\", \"layer2.2.bn2.weight\", \"layer2.2.bn2.bias\", \"layer2.2.bn2.running_mean\", \"layer2.2.bn2.running_var\", \"layer2.2.bn2.num_batches_tracked\", \"layer2.2.conv3.weight\", \"layer2.2.bn3.weight\", \"layer2.2.bn3.bias\", \"layer2.2.bn3.running_mean\", \"layer2.2.bn3.running_var\", \"layer2.2.bn3.num_batches_tracked\", \"layer2.3.conv1.weight\", \"layer2.3.bn1.weight\", \"layer2.3.bn1.bias\", \"layer2.3.bn1.running_mean\", \"layer2.3.bn1.running_var\", \"layer2.3.bn1.num_batches_tracked\", \"layer2.3.conv2.weight\", \"layer2.3.bn2.weight\", \"layer2.3.bn2.bias\", \"layer2.3.bn2.running_mean\", \"layer2.3.bn2.running_var\", \"layer2.3.bn2.num_batches_tracked\", \"layer2.3.conv3.weight\", \"layer2.3.bn3.weight\", \"layer2.3.bn3.bias\", \"layer2.3.bn3.running_mean\", \"layer2.3.bn3.running_var\", \"layer2.3.bn3.num_batches_tracked\", \"layer2.0.conv3.weight\", \"layer2.0.bn3.weight\", \"layer2.0.bn3.bias\", \"layer2.0.bn3.running_mean\", \"layer2.0.bn3.running_var\", \"layer2.0.bn3.num_batches_tracked\", \"layer2.1.conv3.weight\", \"layer2.1.bn3.weight\", \"layer2.1.bn3.bias\", \"layer2.1.bn3.running_mean\", \"layer2.1.bn3.running_var\", \"layer2.1.bn3.num_batches_tracked\", \"layer3.2.conv1.weight\", \"layer3.2.bn1.weight\", \"layer3.2.bn1.bias\", \"layer3.2.bn1.running_mean\", \"layer3.2.bn1.running_var\", \"layer3.2.bn1.num_batches_tracked\", \"layer3.2.conv2.weight\", \"layer3.2.bn2.weight\", \"layer3.2.bn2.bias\", \"layer3.2.bn2.running_mean\", \"layer3.2.bn2.running_var\", \"layer3.2.bn2.num_batches_tracked\", \"layer3.2.conv3.weight\", \"layer3.2.bn3.weight\", \"layer3.2.bn3.bias\", \"layer3.2.bn3.running_mean\", \"layer3.2.bn3.running_var\", \"layer3.2.bn3.num_batches_tracked\", \"layer3.3.conv1.weight\", \"layer3.3.bn1.weight\", \"layer3.3.bn1.bias\", \"layer3.3.bn1.running_mean\", \"layer3.3.bn1.running_var\", \"layer3.3.bn1.num_batches_tracked\", \"layer3.3.conv2.weight\", \"layer3.3.bn2.weight\", \"layer3.3.bn2.bias\", \"layer3.3.bn2.running_mean\", \"layer3.3.bn2.running_var\", \"layer3.3.bn2.num_batches_tracked\", \"layer3.3.conv3.weight\", \"layer3.3.bn3.weight\", \"layer3.3.bn3.bias\", \"layer3.3.bn3.running_mean\", \"layer3.3.bn3.running_var\", \"layer3.3.bn3.num_batches_tracked\", \"layer3.4.conv1.weight\", \"layer3.4.bn1.weight\", \"layer3.4.bn1.bias\", \"layer3.4.bn1.running_mean\", \"layer3.4.bn1.running_var\", \"layer3.4.bn1.num_batches_tracked\", \"layer3.4.conv2.weight\", \"layer3.4.bn2.weight\", \"layer3.4.bn2.bias\", \"layer3.4.bn2.running_mean\", \"layer3.4.bn2.running_var\", \"layer3.4.bn2.num_batches_tracked\", \"layer3.4.conv3.weight\", \"layer3.4.bn3.weight\", \"layer3.4.bn3.bias\", \"layer3.4.bn3.running_mean\", \"layer3.4.bn3.running_var\", \"layer3.4.bn3.num_batches_tracked\", \"layer3.5.conv1.weight\", \"layer3.5.bn1.weight\", \"layer3.5.bn1.bias\", \"layer3.5.bn1.running_mean\", \"layer3.5.bn1.running_var\", \"layer3.5.bn1.num_batches_tracked\", \"layer3.5.conv2.weight\", \"layer3.5.bn2.weight\", \"layer3.5.bn2.bias\", \"layer3.5.bn2.running_mean\", \"layer3.5.bn2.running_var\", \"layer3.5.bn2.num_batches_tracked\", \"layer3.5.conv3.weight\", \"layer3.5.bn3.weight\", \"layer3.5.bn3.bias\", \"layer3.5.bn3.running_mean\", \"layer3.5.bn3.running_var\", \"layer3.5.bn3.num_batches_tracked\", \"layer3.0.conv3.weight\", \"layer3.0.bn3.weight\", \"layer3.0.bn3.bias\", \"layer3.0.bn3.running_mean\", \"layer3.0.bn3.running_var\", \"layer3.0.bn3.num_batches_tracked\", \"layer3.1.conv3.weight\", \"layer3.1.bn3.weight\", \"layer3.1.bn3.bias\", \"layer3.1.bn3.running_mean\", \"layer3.1.bn3.running_var\", \"layer3.1.bn3.num_batches_tracked\", \"layer4.2.conv1.weight\", \"layer4.2.bn1.weight\", \"layer4.2.bn1.bias\", \"layer4.2.bn1.running_mean\", \"layer4.2.bn1.running_var\", \"layer4.2.bn1.num_batches_tracked\", \"layer4.2.conv2.weight\", \"layer4.2.bn2.weight\", \"layer4.2.bn2.bias\", \"layer4.2.bn2.running_mean\", \"layer4.2.bn2.running_var\", \"layer4.2.bn2.num_batches_tracked\", \"layer4.2.conv3.weight\", \"layer4.2.bn3.weight\", \"layer4.2.bn3.bias\", \"layer4.2.bn3.running_mean\", \"layer4.2.bn3.running_var\", \"layer4.2.bn3.num_batches_tracked\", \"layer4.0.conv3.weight\", \"layer4.0.bn3.weight\", \"layer4.0.bn3.bias\", \"layer4.0.bn3.running_mean\", \"layer4.0.bn3.running_var\", \"layer4.0.bn3.num_batches_tracked\", \"layer4.1.conv3.weight\", \"layer4.1.bn3.weight\", \"layer4.1.bn3.bias\", \"layer4.1.bn3.running_mean\", \"layer4.1.bn3.running_var\", \"layer4.1.bn3.num_batches_tracked\". \n\tsize mismatch for layer1.0.conv1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer1.1.conv1.weight: copying a param with shape torch.Size([64, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer2.0.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for layer2.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for layer2.0.downsample.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.downsample.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.downsample.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.downsample.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.1.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for layer3.0.conv1.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).\n\tsize mismatch for layer3.0.downsample.0.weight: copying a param with shape torch.Size([1024, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for layer3.0.downsample.1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.downsample.1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.downsample.1.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.downsample.1.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.1.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for layer4.0.conv1.weight: copying a param with shape torch.Size([512, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 3, 3]).\n\tsize mismatch for layer4.0.downsample.0.weight: copying a param with shape torch.Size([2048, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for layer4.0.downsample.1.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer4.0.downsample.1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer4.0.downsample.1.running_mean: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer4.0.downsample.1.running_var: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer4.1.conv1.weight: copying a param with shape torch.Size([512, 2048, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([10, 2048]) from checkpoint, the shape in current model is torch.Size([10, 512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-ed27505c42fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mencoder2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloaded_dict_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnist_cnn.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mencoder2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_dict_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/zhao/teaching/samhitha.m-ML/resnet18/.venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1045\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tUnexpected key(s) in state_dict: \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.bn1.bias\", \"layer1.2.bn1.running_mean\", \"layer1.2.bn1.running_var\", \"layer1.2.bn1.num_batches_tracked\", \"layer1.2.conv2.weight\", \"layer1.2.bn2.weight\", \"layer1.2.bn2.bias\", \"layer1.2.bn2.running_mean\", \"layer1.2.bn2.running_var\", \"layer1.2.bn2.num_batches_tracked\", \"layer1.2.conv3.weight\", \"layer1.2.bn3.weight\", \"layer1.2.bn3.bias\", \"layer1.2.bn3.running_mean\", \"layer1.2.bn3.running_var\", \"layer1.2.bn3.num_batches_tracked\", \"layer1.0.conv3.weight\", \"layer1.0.bn3.weight\", \"layer1.0.bn3.bias\", \"layer1.0.bn3.running_mean\", \"layer1.0.bn3.running_var\", \"layer1.0.bn3.num_batches_tracked\", \"layer1.0.downsample.0.weight\", \"layer1.0.downsample.1.weight\", \"layer1.0.downsample.1.bias\", \"layer1.0.downsample.1.running_mean\", \"layer1.0.downsample.1.running_var\", \"layer1.0.downsample.1.num_batches_tracked\", \"layer1.1.conv3.weight\", \"layer1.1.bn3.weight\", \"layer1.1.bn3.bias\", \"layer1.1.bn3.running_mean\", \"layer1.1.bn3.running_var\", \"layer1.1.bn3.num_batches_tracked\", \"layer2.2.conv1.weight\", \"layer2.2.bn1.weight\", \"layer2.2.bn1.bias\", \"layer2.2.bn1.running_mean\", \"layer2.2.bn1.running_var\", \"layer2.2.bn1.num_batches_tracked\", \"layer2.2.conv2.weight\", \"layer2.2.bn2.weight\", \"layer2.2.bn2.bias\", \"layer2.2.bn2.running_mean\", \"layer2.2.bn2.running_var\", \"layer2.2.bn2.num_batches_tracked\", \"layer2.2.conv3.weight\", \"layer2.2.bn3.weight\", \"layer2.2.bn3.bias\", \"layer2.2.bn3.running_mean\", \"layer2.2.bn3.running_var\", \"layer2.2.bn3.num_batches_tracked\", \"layer2.3.conv1.weight\", \"layer2.3.bn1.weight\", \"layer2.3.bn1.bias\", \"layer2.3.bn1.running_mean\", \"layer2.3.bn1.running_var\", \"layer2.3.bn1.num_batches_tracked\", \"layer2.3.conv2.weight\", \"layer2.3.bn2.weight\", \"layer2.3.bn2.bias\", \"layer2.3.bn2.running_mean\", \"layer2.3.bn2.running_var\", \"layer2.3.bn2.num_batches_tracked\", \"layer2.3.conv3.weight\", \"layer2.3.bn3.weight\", \"layer2.3.bn3.bias\", \"layer2.3.bn3.running_mean\", \"layer2.3.bn3.running_var\", \"layer2.3.bn3.num_batches_tracked\", \"layer2.0.conv3.weight\", \"layer2.0.bn3.weight\", \"layer2.0.bn3.bias\", \"layer2.0.bn3.running_mean\", \"layer2.0.bn3.running_var\", \"layer2.0.bn3.num_batches_tracked\", \"layer2.1.conv3.weight\", \"layer2.1.bn3.weight\", \"layer2.1.bn3.bias\", \"layer2.1.bn3.running_mean\", \"layer2.1.bn3.running_var\", \"layer2.1.bn3.num_batches_tracked\", \"layer3.2.conv1.weight\", \"layer3.2.bn1.weight\", \"layer3.2.bn1.bias\", \"layer3.2.bn1.running_mean\", \"layer3.2.bn1.running_var\", \"layer3.2.bn1.num_batches_tracked\", \"layer3.2.conv2.weight\", \"layer3.2.bn2.weight\", \"layer3.2.bn2.bias\", \"layer3.2.bn2.running_mean\", \"layer3.2.bn2.running_var\", \"layer3.2.bn2.num_batches_tracked\", \"layer3.2.conv3.weight\", \"layer3.2.bn3.weight\", \"layer3.2.bn3.bias\", \"layer3.2.bn3.running_mean\", \"layer3.2.bn3.running_var\", \"layer3.2.bn3.num_batches_tracked\", \"layer3.3.conv1.weight\", \"layer3.3.bn1.weight\", \"layer3.3.bn1.bias\", \"layer3.3.bn1.running_mean\", \"layer3.3.bn1.running_var\", \"layer3.3.bn1.num_batches_tracked\", \"layer3.3.conv2.weight\", \"layer3.3.bn2.weight\", \"layer3.3.bn2.bias\", \"layer3.3.bn2.running_mean\", \"layer3.3.bn2.running_var\", \"layer3.3.bn2.num_batches_tracked\", \"layer3.3.conv3.weight\", \"layer3.3.bn3.weight\", \"layer3.3.bn3.bias\", \"layer3.3.bn3.running_mean\", \"layer3.3.bn3.running_var\", \"layer3.3.bn3.num_batches_tracked\", \"layer3.4.conv1.weight\", \"layer3.4.bn1.weight\", \"layer3.4.bn1.bias\", \"layer3.4.bn1.running_mean\", \"layer3.4.bn1.running_var\", \"layer3.4.bn1.num_batches_tracked\", \"layer3.4.conv2.weight\", \"layer3.4.bn2.weight\", \"layer3.4.bn2.bias\", \"layer3.4.bn2.running_mean\", \"layer3.4.bn2.running_var\", \"layer3.4.bn2.num_batches_tracked\", \"layer3.4.conv3.weight\", \"layer3.4.bn3.weight\", \"layer3.4.bn3.bias\", \"layer3.4.bn3.running_mean\", \"layer3.4.bn3.running_var\", \"layer3.4.bn3.num_batches_tracked\", \"layer3.5.conv1.weight\", \"layer3.5.bn1.weight\", \"layer3.5.bn1.bias\", \"layer3.5.bn1.running_mean\", \"layer3.5.bn1.running_var\", \"layer3.5.bn1.num_batches_tracked\", \"layer3.5.conv2.weight\", \"layer3.5.bn2.weight\", \"layer3.5.bn2.bias\", \"layer3.5.bn2.running_mean\", \"layer3.5.bn2.running_var\", \"layer3.5.bn2.num_batches_tracked\", \"layer3.5.conv3.weight\", \"layer3.5.bn3.weight\", \"layer3.5.bn3.bias\", \"layer3.5.bn3.running_mean\", \"layer3.5.bn3.running_var\", \"layer3.5.bn3.num_batches_tracked\", \"layer3.0.conv3.weight\", \"layer3.0.bn3.weight\", \"layer3.0.bn3.bias\", \"layer3.0.bn3.running_mean\", \"layer3.0.bn3.running_var\", \"layer3.0.bn3.num_batches_tracked\", \"layer3.1.conv3.weight\", \"layer3.1.bn3.weight\", \"layer3.1.bn3.bias\", \"layer3.1.bn3.running_mean\", \"layer3.1.bn3.running_var\", \"layer3.1.bn3.num_batches_tracked\", \"layer4.2.conv1.weight\", \"layer4.2.bn1.weight\", \"layer4.2.bn1.bias\", \"layer4.2.bn1.running_mean\", \"layer4.2.bn1.running_var\", \"layer4.2.bn1.num_batches_tracked\", \"layer4.2.conv2.weight\", \"layer4.2.bn2.weight\", \"layer4.2.bn2.bias\", \"layer4.2.bn2.running_mean\", \"layer4.2.bn2.running_var\", \"layer4.2.bn2.num_batches_tracked\", \"layer4.2.conv3.weight\", \"layer4.2.bn3.weight\", \"layer4.2.bn3.bias\", \"layer4.2.bn3.running_mean\", \"layer4.2.bn3.running_var\", \"layer4.2.bn3.num_batches_tracked\", \"layer4.0.conv3.weight\", \"layer4.0.bn3.weight\", \"layer4.0.bn3.bias\", \"layer4.0.bn3.running_mean\", \"layer4.0.bn3.running_var\", \"layer4.0.bn3.num_batches_tracked\", \"layer4.1.conv3.weight\", \"layer4.1.bn3.weight\", \"layer4.1.bn3.bias\", \"layer4.1.bn3.running_mean\", \"layer4.1.bn3.running_var\", \"layer4.1.bn3.num_batches_tracked\". \n\tsize mismatch for layer1.0.conv1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer1.1.conv1.weight: copying a param with shape torch.Size([64, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer2.0.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for layer2.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for layer2.0.downsample.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.downsample.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.downsample.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.downsample.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.1.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for layer3.0.conv1.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).\n\tsize mismatch for layer3.0.downsample.0.weight: copying a param with shape torch.Size([1024, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for layer3.0.downsample.1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.downsample.1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.downsample.1.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.downsample.1.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.1.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for layer4.0.conv1.weight: copying a param with shape torch.Size([512, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 3, 3]).\n\tsize mismatch for layer4.0.downsample.0.weight: copying a param with shape torch.Size([2048, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for layer4.0.downsample.1.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer4.0.downsample.1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer4.0.downsample.1.running_mean: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer4.0.downsample.1.running_var: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer4.1.conv1.weight: copying a param with shape torch.Size([512, 2048, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([10, 2048]) from checkpoint, the shape in current model is torch.Size([10, 512])."
     ]
    }
   ],
   "source": [
    "encoder2.load_state_dict(loaded_dict_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
